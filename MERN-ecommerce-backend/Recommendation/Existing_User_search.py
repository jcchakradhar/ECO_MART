import pandas as pd
from .common_code import calculate_product_score, vectorizer, tag_vectors, get_user_avg_price
from sklearn.metrics.pairwise import cosine_similarity
from .common_code import search_history, purchase_history, weights, price_tolerance
from .common_code import remove_similar_items
from .workable_data import workable_dataset, product_vectorizer, product_vectors, category_vectorizer, category_vectors
# from common_code import user_profile


# def search_based_recommendation(user_profile,query, df):
#     weights = user_profile["weights"]
#     purchase_history = user_profile["purchase_history"]
#     price_tolerance = user_profile["price_tolerance"]
    
#     # --- Step 1: Vectorize the query ---
#     query_vec = vectorizer.transform([query])
    
    
#     # --- Step 2: Compute cosine similarity between query and tag_vectors ---
#     similarities = cosine_similarity(query_vec, tag_vectors).flatten()
#     df_copy = df.copy()
#     df_copy["similarity"] = similarities
    
#     # --- Step 3: Filter based on price tolerance ---
#     # if purchase_history:
#     #     purchased_df = df[df["Item Number"].isin(purchase_history)]
#     #     avg_price = get_user_avg_price(purchased_df, df)
#     #     print(avg_price)
#     #     if avg_price:
#     #         min_price = avg_price * (1 - price_tolerance)
#     #         max_price = avg_price * (1 + price_tolerance)
#     #         df_copy = df_copy[(df_copy["Sale Price"] >= min_price) & (df_copy["Sale Price"] <= max_price)]
#     # print(df_copy.shape)
    
#      # --- Step 4: Compute sustainability score ---
#     df_copy["sustainability_score"] = df_copy.apply(
#         lambda row: calculate_product_score(row, weights), axis=1
#     )

#     # --- Step 5: Final score (combine both) ---
#     df_copy["final_score"] = (
#         0.6 * df_copy["similarity"] + 
#         0.4 * (df_copy["sustainability_score"] / 5)
#     )

#     # --- Step 6: Remove already purchased or similar items ---
#     # purchased_names = df[df["Item Number"].isin(purchase_history)]["Product Name"].tolist()
#     # if purchased_names:
#     #     name_vecs = vectorizer.transform(df_copy["Product Name"])
#     #     purchased_vecs = vectorizer.transform(purchased_names)
#     #     sim_matrix = cosine_similarity(name_vecs, purchased_vecs)
#     #     max_sim = sim_matrix.max(axis=1)
#     #     df_copy = df_copy[max_sim < similarity_threshold]

#     # --- Step 7: Return Top K Recommendations ---
#     df_copy = df_copy.sort_values("final_score", ascending=False)
#     return df_copy["_id"]

# # ...existing code...
# import pandas as pd
# from .common_code import calculate_product_score  # ensure relative import

# def search_based_recommendation(profile, query, df):
#     q = (query or "").strip()
#     if not q:
#         return []

#     if not isinstance(df, pd.DataFrame) or df.empty:
#         return []

#     # Choose available text columns to search
#     text_cols = [c for c in ['title', 'name', 'product_name', 'description', 'category_name', 'tags']
#                  if c in df.columns]
#     if not text_cols:
#         return []

#     # Build a boolean mask (avoid “if Series”)
#     mask = pd.Series(False, index=df.index)
#     for col in text_cols:
#         mask = mask | df[col].astype(str).str.contains(q, case=False, na=False)

#     matches = df.loc[mask].copy()
#     if matches.empty:
#         return []

#     # Optional scoring using user weights; fall back if it fails
#     try:
#         weights = (profile or {}).get('weights', {}) if isinstance(profile, dict) else {}
#         matches['__score'] = matches.apply(lambda row: calculate_product_score(row, weights), axis=1)
#         matches = matches.sort_values('__score', ascending=False)
#     except Exception:
#         if 'price' in matches.columns:
#             matches = matches.sort_values('price', ascending=True)

#     # Select output columns and sanitize IDs
#     id_col = '_id' if '_id' in matches.columns else ('product_id' if 'product_id' in matches.columns else None)
#     out_cols = [id_col]
#     out = matches[out_cols].head(20)

#     from bson import ObjectId, Binary
#     from datetime import datetime
#     import base64

#     def _to_jsonable(v):
#         if isinstance(v, ObjectId):
#             return str(v)
#         if isinstance(v, (bytes, bytearray)):
#             return base64.b64encode(v).decode('ascii')
#         if isinstance(v, Binary):
#             return base64.b64encode(bytes(v)).decode('ascii')
#         if isinstance(v, datetime):
#             return v.isoformat()
#         return v

#     return [{k: _to_jsonable(v) for k, v in rec.items()} for rec in out.to_dict(orient='records')]
# # ...existing code...

# ...existing code...
import pandas as pd
from .common_code import calculate_product_score  # relative import
from sklearn.metrics.pairwise import cosine_similarity
from bson import ObjectId, Binary
from datetime import datetime
import base64


def _to_jsonable(v):
    """Helper: safely convert non-JSON-friendly values."""
    if isinstance(v, ObjectId):
        return str(v)
    if isinstance(v, (bytes, bytearray)):
        return base64.b64encode(v).decode("ascii")
    if isinstance(v, Binary):
        return base64.b64encode(bytes(v)).decode("ascii")
    if isinstance(v, datetime):
        return v.isoformat()
    return v


def search_based_recommendation(profile, query, df):
    q = (query or "").strip()
    if not q:
        return []

    if not isinstance(df, pd.DataFrame) or df.empty:
        return []

    # --- Step 1: Vectorize query ---
    try:
        query_vec = vectorizer.transform([q])
    except Exception as e:
        return [{"error": f"vectorizer failed: {e}"}]

    # --- Step 2: Compute cosine similarity ---
    try:
        # Step 1: Transform the query into vector
        query_vec = vectorizer.transform([query])
        # sim_scores = cosine_similarity(query_vec, tag_vectors).flatten()
        
        # Step 2: Transform the query using both vectorizers
        product_vec = product_vectorizer.transform([query])
        category_vec = category_vectorizer.transform([query])

        # Step 3: Compute similarities separately
        sim_product = cosine_similarity(product_vec, product_vectors).flatten()
        sim_category = cosine_similarity(category_vec, category_vectors).flatten()

        product_weight = 0.7
        category_weight = 0.3
        # Step 4: Weighted similarity
        sim_scores = product_weight * sim_product + category_weight * sim_category
    except Exception as e:
        return [{"error": f"cosine similarity failed: {e}"}]

    df_copy = df.copy()
    df_copy["similarity"] = sim_scores

    # --- Step 3: Price tolerance filter ---
    weights = (profile or {}).get("weights", {}) if isinstance(profile, dict) else {}
    # purchase_history = (profile or {}).get("purchase_history", [])
    # price_tolerance = (profile or {}).get("price_tolerance", 0.2)

    # if purchase_history:
    #     purchased_df = df[df["Item Number"].isin(purchase_history)]
    #     avg_price = get_user_avg_price(purchased_df, df)
    #     if avg_price:
    #         min_price, max_price = avg_price * (1 - price_tolerance), avg_price * (1 + price_tolerance)
    #         df_copy = df_copy[
    #             (df_copy["Sale Price"] >= min_price) & (df_copy["Sale Price"] <= max_price)
    #         ]

    if df_copy.empty:
        return []

    # --- Step 4: Sustainability scoring ---
    try:
        df_copy["sustainability_score"] = df_copy.apply(
            lambda row: calculate_product_score(row, weights), axis=1
        )
    except Exception:
        df_copy["sustainability_score"] = 0

    # --- Step 5: Final score (weighted similarity + sustainability) ---
    df_copy["final_score"] = (
        0.6 * df_copy["similarity"] + 0.4 * (df_copy["sustainability_score"] / 5)
    )

    # --- Step 6: Remove already purchased or near-duplicates ---
    # if purchase_history:
    #     purchased_names = df[df["Item Number"].isin(purchase_history)]["Product Name"].tolist()
    #     if purchased_names:
    #         try:
    #             name_vecs = vectorizer.transform(df_copy["Product Name"])
    #             purchased_vecs = vectorizer.transform(purchased_names)
    #             sim_matrix = cosine_similarity(name_vecs, purchased_vecs)
    #             max_sim = sim_matrix.max(axis=1)
    #             df_copy = df_copy[max_sim < similarity_threshold]
    #         except Exception:
    #             pass

    if df_copy.empty:
        return []

    # --- Step 7: Sort + Select top-k ---
    # df_copy = df_copy.sort_values("final_score", ascending=False)
    # id_col = "_id" if "_id" in df_copy.columns else (
    #     "product_id" if "product_id" in df_copy.columns else None
    # )

    # if not id_col:
    #     return []

    # out = df_copy[[id_col]].head(20)

    id_col = "_id" if "_id" in df_copy.columns else (
        "product_id" if "product_id" in df_copy.columns else None
    )

    if not id_col:
        return []

    # Collect the desired output columns (if they exist)
    # out_cols = [id_col]
    # for col in ["title", "product_id"]:
    #     if col in df_copy.columns:
    #         out_cols.append(col)

    # # Slice dataframe
    # out = df_copy[out_cols].head(20)

    # # --- Step 8: Make JSON-serializable ---
    # return [{k: _to_jsonable(v) for k, v in rec.items()} for rec in out.to_dict(orient="records")]

    out_ids = df_copy[id_col].head(20).tolist()
    return [_to_jsonable(v) for v in out_ids]
